{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Continual Learning on Split CIFAR-10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "\n",
    "example_dir_path = Path().resolve()\n",
    "data_dir_path = Path.joinpath(example_dir_path, \"data\")\n",
    "\n",
    "split_cifar10 = SplitCIFAR10(\n",
    "    n_experiences=5,\n",
    "    dataset_root=Path.joinpath(data_dir_path, \"cifar10\"),\n",
    "    shuffle=False,\n",
    "    return_task_id=True,\n",
    "    class_ids_from_zero_in_each_exp=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T18:13:12.412606Z",
     "start_time": "2023-07-14T18:13:09.164170Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Lightning Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Must import `hat.networks` to register the models\n",
    "# noinspection PyUnresolvedReferences\n",
    "import hat.timm_models\n",
    "from hat import HATConfig, HATPayload\n",
    "from hat.utils import get_hat_reg_term, get_hat_mask_scale\n",
    "\n",
    "\n",
    "class ContinualClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_strat,\n",
    "        scaling_strat,\n",
    "        num_classes_per_exp,\n",
    "        max_mask_scale=100.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_strat = init_strat\n",
    "        self.scaling_strat = scaling_strat\n",
    "        self.num_classes_per_exp = num_classes_per_exp\n",
    "        self.max_mask_scale = max_mask_scale\n",
    "        _hat_config = HATConfig(\n",
    "            init_strat=self.init_strat,\n",
    "            num_tasks=len(num_classes_per_exp),\n",
    "        )\n",
    "        self.backbone = timm.create_model(\n",
    "            \"hat_resnet18s\",\n",
    "            num_classes=0,\n",
    "            hat_config=_hat_config,\n",
    "        )\n",
    "        self.heads = nn.ModuleList(\n",
    "            [nn.Linear(512, __c) for __c in num_classes_per_exp]\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, images, task_id, mask_scale=None):\n",
    "        _pld = HATPayload(images, task_id=task_id, mask_scale=mask_scale)\n",
    "        return self.heads[_pld.task_id](self.backbone(_pld).data)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _images, _targets, _task_id = batch\n",
    "        _progress = batch_idx / (self.trainer.num_training_batches - 1)\n",
    "        _mask_scale = get_hat_mask_scale(\n",
    "            strat=self.scaling_strat,\n",
    "            max_trn_mask_scale=self.max_mask_scale,\n",
    "            progress=_progress,\n",
    "        )\n",
    "        _logits = self.forward(_images, _task_id, _mask_scale)\n",
    "        _loss = self.criterion(_logits, _targets)\n",
    "        _reg = get_hat_reg_term(\n",
    "            module=self.backbone,\n",
    "            strat=\"uniform\",\n",
    "            task_id=_task_id,\n",
    "            mask_scale=_mask_scale,\n",
    "            # forgive_quota=False,\n",
    "        )\n",
    "        return _loss + _reg\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx):\n",
    "        _images, _targets, _task_id = batch\n",
    "        # Class-incremental learning\n",
    "        # Iterate through all the tasks and compute the logits\n",
    "        _logits = []\n",
    "        for __task_id in range(len(self.heads)):\n",
    "            _logits.append(\n",
    "                self.forward(_images, __task_id, self.max_mask_scale)\n",
    "            )\n",
    "        # Class-incremental testing\n",
    "        _cil_logits = torch.cat(_logits, dim=1)\n",
    "        _cil_targets = _targets + sum(self.num_classes_per_exp[:_task_id])\n",
    "        _cil_acc = _cil_logits.argmax(dim=1) == _cil_targets\n",
    "        # Task-incremental testing\n",
    "        _til_logits = _logits[_task_id]\n",
    "        _til_acc = _til_logits.argmax(dim=1) == _targets\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"cil_acc\": _cil_acc.float().mean(),\n",
    "                \"til_acc\": _til_acc.float().mean(),\n",
    "            },\n",
    "            batch_size=_images.shape[0],\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Bigger learning rate or more epochs may be needed if\n",
    "        # the model is using dense initialization for HAT maskers.\n",
    "        # return torch.optim.SGD(self.parameters(), lr=1e-3)\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T18:13:13.023379Z",
     "start_time": "2023-07-14T18:13:12.415563Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the Model for Each Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92f160ac95644a51849a791e1d41e6a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ec2d2495a954299bca3e682eef90137"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a90d3882c6e42f490ddc9d3d2a9a86c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 3\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06996ce9949c48fbbc6f17f93606db1c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93887c30ecf447e0b718e307c09dfd7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"pytorch_lightning\")\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "logger = logging.getLogger(\"lightning_fabric\")\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets, task_ids = zip(*batch)\n",
    "    return (torch.stack(images), torch.tensor(targets), int(task_ids[0]))\n",
    "\n",
    "\n",
    "clf = ContinualClassifier(\n",
    "    init_strat=\"dense\",\n",
    "    scaling_strat=\"cosine\",\n",
    "    num_classes_per_exp=split_cifar10.n_classes_per_exp,\n",
    ")\n",
    "accelerator = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "strategy = (\n",
    "    \"ddp_notebook_find_unused_parameters_true\"\n",
    "    if accelerator == \"cuda\"\n",
    "    else None\n",
    ")\n",
    "\n",
    "\n",
    "for __task_id, __trn_exp in enumerate(split_cifar10.train_stream):\n",
    "    print(f\"Training on task/experience {__task_id}\")\n",
    "    _trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=accelerator,\n",
    "        # strategy=strategy,\n",
    "        devices=1,\n",
    "    )\n",
    "    _dataloader = DataLoader(\n",
    "        __trn_exp.dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=8 if accelerator == \"cuda\" else 0,\n",
    "        pin_memory=True if accelerator == \"cuda\" else False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    _trainer.fit(clf, _dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T18:16:35.066320Z",
     "start_time": "2023-07-14T18:13:13.028127Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ca458541f854c9a9596925a5e352c25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.freeze()\n",
    "for __m in clf.modules():\n",
    "    if isinstance(__m, nn.BatchNorm2d):\n",
    "        __m.track_running_stats = False\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=accelerator,\n",
    "    devices=1,\n",
    "    enable_model_summary=False,\n",
    ")\n",
    "tst_dataloaders = [\n",
    "    DataLoader(\n",
    "        __exp.dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=8 if accelerator == \"cuda\" else 0,\n",
    "        pin_memory=True if accelerator == \"cuda\" else False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    for __exp in split_cifar10.test_stream\n",
    "]\n",
    "tst_results = trainer.test(clf, tst_dataloaders, verbose=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T18:16:41.461367Z",
     "start_time": "2023-07-14T18:16:35.063665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤══════════╤══════════╤══════════╤══════════╤══════════╤════════╕\n",
      "│     │  Task 0  │  Task 1  │  Task 2  │  Task 3  │  Task 4  │  Avg   │\n",
      "╞═════╪══════════╪══════════╪══════════╪══════════╪══════════╪════════╡\n",
      "│ CIL │  30.80%  │  26.85%  │  24.60%  │  69.65%  │  72.70%  │ 44.92% │\n",
      "│ TIL │  92.10%  │  77.05%  │  91.90%  │  96.80%  │  95.45%  │ 90.66% │\n",
      "╘═════╧══════════╧══════════╧══════════╧══════════╧══════════╧════════╛\n"
     ]
    }
   ],
   "source": [
    "# Reformat results for better readability\n",
    "import pandas as pd\n",
    "\n",
    "reformatted_tst_results = {}\n",
    "til_acc, cil_acc = [], []\n",
    "for __tst_results in tst_results:\n",
    "    for __label, __acc in __tst_results.items():\n",
    "        __il, __dl = __label.split(\"/\")\n",
    "        __il = __il.split(\"_\")[0].upper()\n",
    "        __dl = f\"Task {__dl.split('_')[-1]}\"\n",
    "        if __dl not in reformatted_tst_results:\n",
    "            reformatted_tst_results[__dl] = {}\n",
    "        reformatted_tst_results[__dl][__il] = __acc\n",
    "        if __il == \"TIL\":\n",
    "            til_acc.append(__acc)\n",
    "        else:\n",
    "            cil_acc.append(__acc)\n",
    "reformatted_tst_results[\"Avg\"] = {\n",
    "    \"TIL\": sum(til_acc) / len(til_acc),\n",
    "    \"CIL\": sum(cil_acc) / len(cil_acc),\n",
    "}\n",
    "reformatted_tst_results = pd.DataFrame(reformatted_tst_results)\n",
    "print(\n",
    "    reformatted_tst_results.to_markdown(\n",
    "        floatfmt=\"2.2%\",\n",
    "        tablefmt=\"fancy_outline\",\n",
    "        stralign=\"center\",\n",
    "        numalign=\"center\",\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T18:16:41.506584Z",
     "start_time": "2023-07-14T18:16:41.504438Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hat-cl",
   "language": "python",
   "display_name": "Python (hat-cl)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
