{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Continual Learning on Split CIFAR-10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "\n",
    "example_dir_path = Path().resolve()\n",
    "data_dir_path = Path.joinpath(example_dir_path, \"data\")\n",
    "\n",
    "split_cifar10 = SplitCIFAR10(\n",
    "    n_experiences=5,\n",
    "    dataset_root=Path.joinpath(data_dir_path, \"cifar10\"),\n",
    "    shuffle=False,\n",
    "    return_task_id=True,\n",
    "    class_ids_from_zero_in_each_exp=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Lightning Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Must import `hat.networks` to register the models\n",
    "# noinspection PyUnresolvedReferences\n",
    "import hat.networks\n",
    "from hat import HATConfig, HATPayload\n",
    "from hat.utils import get_hat_reg_term\n",
    "\n",
    "\n",
    "class ContinualClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes_per_exp, max_mask_scale=100.0):\n",
    "        super().__init__()\n",
    "        self.num_classes_per_exp = num_classes_per_exp\n",
    "        self.max_mask_scale = max_mask_scale\n",
    "        _hat_config = HATConfig(\n",
    "            num_tasks=len(num_classes_per_exp),\n",
    "        )\n",
    "        self.backbone = timm.create_model(\n",
    "            \"hat_resnet18s\",\n",
    "            num_classes=0,\n",
    "            hat_config=_hat_config,\n",
    "        )\n",
    "        self.heads = nn.ModuleList(\n",
    "            [nn.Linear(512, __c) for __c in num_classes_per_exp]\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, images, task_id, mask_scale=None):\n",
    "        pld = HATPayload(images, task_id=task_id, mask_scale=mask_scale)\n",
    "        return self.heads[pld.task_id](self.backbone(pld).data)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets, task_id = batch\n",
    "        # Progress is the percentage of the training completed\n",
    "        _progress = (batch_idx + 1) / self.trainer.num_training_batches\n",
    "        _mask_scale = _progress * self.max_mask_scale\n",
    "        logits = self.forward(images, task_id, _mask_scale)\n",
    "        loss = self.criterion(logits, targets)\n",
    "        reg = get_hat_reg_term(\n",
    "            module=self.backbone,\n",
    "            reg_strategy=\"uniform\",\n",
    "            task_id=task_id,\n",
    "            mask_scale=_mask_scale,\n",
    "        )\n",
    "        return loss + reg\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx):\n",
    "        images, targets, task_id = batch\n",
    "        # Class-incremental learning\n",
    "        # Iterate through all the tasks and compute the logits\n",
    "        logits = []\n",
    "        for __task_id in range(len(self.heads)):\n",
    "            logits.append(self.forward(images, __task_id, self.max_mask_scale))\n",
    "        # Class-incremental testing\n",
    "        cil_logits = torch.cat(logits, dim=1)\n",
    "        cil_targets = targets + sum(self.num_classes_per_exp[:task_id])\n",
    "        cil_acc = cil_logits.argmax(dim=1) == cil_targets\n",
    "        # Task-incremental testing\n",
    "        til_logits = logits[task_id]\n",
    "        til_acc = til_logits.argmax(dim=1) == targets\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"cil_acc\": cil_acc.float().mean(),\n",
    "                \"til_acc\": til_acc.float().mean(),\n",
    "            },\n",
    "            batch_size=images.shape[0],\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the Model for Each Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | HATResNet        | 11.2 M\n",
      "1 | heads     | ModuleList       | 5.1 K \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "100       Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.984    Total estimated model params size (MB)\n",
      "/homes/duan/software/anaconda3/envs/hat-cl/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf910bb8e9994a03b7800636b80ea676"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | HATResNet        | 11.2 M\n",
      "1 | heads     | ModuleList       | 5.1 K \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "100       Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.984    Total estimated model params size (MB)\n",
      "/homes/duan/software/anaconda3/envs/hat-cl/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "787563da2c5f4b3d98e757b4b646de53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | HATResNet        | 11.2 M\n",
      "1 | heads     | ModuleList       | 5.1 K \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "100       Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.984    Total estimated model params size (MB)\n",
      "/homes/duan/software/anaconda3/envs/hat-cl/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a24265bed87d4c4094a8cc58eeb25760"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | HATResNet        | 11.2 M\n",
      "1 | heads     | ModuleList       | 5.1 K \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "100       Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.984    Total estimated model params size (MB)\n",
      "/homes/duan/software/anaconda3/envs/hat-cl/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bef11b65081c4b8185f79af59c5cf9a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task/experience 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | HATResNet        | 11.2 M\n",
      "1 | heads     | ModuleList       | 5.1 K \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "100       Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.984    Total estimated model params size (MB)\n",
      "/homes/duan/software/anaconda3/envs/hat-cl/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26bcc1f7fda2422cab31857d7e570cdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "clf = ContinualClassifier(split_cifar10.n_classes_per_exp)\n",
    "device = \"cuda\"\n",
    "strategy = \"ddp_notebook_find_unused_parameters_true\"\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets, task_ids = zip(*batch)\n",
    "    return (torch.stack(images), torch.tensor(targets), int(task_ids[0]))\n",
    "\n",
    "\n",
    "for __task_id, __trn_exp in enumerate(split_cifar10.train_stream):\n",
    "    print(f\"Training on task/experience {__task_id}\")\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=device,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        __trn_exp.dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True if device == \"cuda\" else False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    trainer.fit(clf, dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d465a7b29504a16b62e38ccd3ad99b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 1       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 2       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m         cil_acc         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6290000081062317    \u001B[0m\u001B[35m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.06949999928474426   \u001B[0m\u001B[35m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.2930000126361847    \u001B[0m\u001B[35m \u001B[0m│\n│\u001B[36m \u001B[0m\u001B[36m         til_acc         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.9465000033378601    \u001B[0m\u001B[35m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.8355000019073486    \u001B[0m\u001B[35m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.8654999732971191    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃<span style=\"font-weight: bold\">       DataLoader 1        </span>┃<span style=\"font-weight: bold\">       DataLoader 2        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">          cil_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6290000081062317     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.06949999928474426    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2930000126361847     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">          til_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9465000033378601     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8355000019073486     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8654999732971191     </span>│\n└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 3       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 4       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m         cil_acc         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6589999794960022    \u001B[0m\u001B[35m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.45249998569488525   \u001B[0m\u001B[35m \u001B[0m│\n│\u001B[36m \u001B[0m\u001B[36m         til_acc         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m         0.9375          \u001B[0m\u001B[35m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.9300000071525574    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 3        </span>┃<span style=\"font-weight: bold\">       DataLoader 4        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">          cil_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6589999794960022     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.45249998569488525    </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">          til_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.9375           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9300000071525574     </span>│\n└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.freeze()\n",
    "for __m in clf.modules():\n",
    "    if isinstance(__m, nn.BatchNorm2d):\n",
    "        __m.track_running_stats = False\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=device,\n",
    "    devices=1,\n",
    ")\n",
    "tst_dataloaders = [\n",
    "    DataLoader(\n",
    "        __exp.dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        pin_memory=True if device == \"cuda\" else False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    for __exp in split_cifar10.test_stream\n",
    "]\n",
    "tst_results = trainer.test(clf, tst_dataloaders)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
